---
title: 'MUSA500 Homework2: Using OLS Regression to Predict Median House Values in
  Philadelphia'
author: "Ling Chen, Hang Zhao, Jiahang Li"
date: "2023-10-16"
output:
  html_document:
  toc: yes
toc_float: yes
code_folding: hide
number_sections: yes
code_download: no
theme: united
highlight: espresso
pdf_document:
  toc: yes
editor_options:
  markdown:
  wrap: 72
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

```{r setup2, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set()

library(sf)
#library(rgeos)
library(spdep)
library(spgwr)
library(tmap)
library(spatialreg)
library(whitestrap)
library(lmtest)
library(tseries)

options(scipen=999)

data <- read.csv("C:/Users/jiahangl/OneDrive - PennO365/data mining/A1/RegressionData.csv")
data_geom <-st_read("C:/Users/jiahangl/OneDrive - PennO365/data mining/A1/Lecture 1 - RegressionData.shp.zip")

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```


# **Introduction**

Philadelphia, celebrated for its historical depth and vibrant present, boasts an ever-changing real estate landscape. Yet, a report from the Economic League paints a more intricate image: between 2016 and 2021, the percentage of Philadelphia households struggling with housing costs dipped from 29.8% to 26.7% (Economic League, 2023). Given the essential nature of housing as a basic human need, ensuring its affordability is paramount to sustaining quality of life. Thus, understanding the elements that shape housing prices is key to navigating the housing market more effectively and making wiser, informed choices.

In our previous exploration of Philadelphia's housing landscape, we used Ordinary Least Squares (OLS) regression to examine the relationship between median house value and several neighborhood characteristics, including the proportion of residents in the Block Group with at least a bachelor’s degree, housing vacancy, percentage of housing units that are detached single-family houses, and number of households living poverty. While OLS provided valuable insights, it operates on the assumption of no autocorrelation. However, the real world, especially in the domain of housing and neighborhood dynamics, often defies this assumption as there is the phenomena of spatial autocorrelation, which can lead to biased and inefficient estimates if not addressed in regression models. To confront this inherent spatial nature of our data, in this report, we will venture into spatial lag, spatial error, and geographically weighted regression methodologies to understand if these models can better account for the spatial dependencies lurking in our OLS residuals, offering a more holistic and accurate picture of Philadelphia's housing valuation dynamics.
 


# **Methods**
## a)	A Description of the Concept of Spatial Autocorrelation
There's a saying: "everything is related to everything else, but near things are more related than distant things." This adage, known as Tobler's First Law of Geography, brings to the fore the concept of spatial autocorrelation – the phenomenon where geographically close observations influence each other. In other words, the attributes of places (or events) become more dissimilar as they are located further apart.

To evaluate spatial autocorrelation, we use Moran's I, a correlation coefficient that measures the spatial relationships within a dataset. Essentially, it assesses the similarity of an object to its neighbors.

Turning to the formula of Moran’s I, the Moran’s I can be calculated as:
$$
I=\frac{N\sum_{i = 1}\sum_{j = 1}^{n}wij{(x_i-\bar{x})(x_j-\bar{x})}}{\sum_{i = 1}\sum_{j = 1}^{n}wij{\sum_{i = 1}^{n}{(x_i-\bar{x})^2}}}
$$
In this formula:
N is the number of spatial units
Xi is the variable of interest.
Xj is the variable value at another location j
X- is the mean of the variable X
wij is a matrix of spatial weights between spatial units i and j

In this report, the weight matrix we use is queen contiguity. Based on this spatial matrix, a unit is considered adjacent (or a neighbor) to another if it shares either a border or a vertex (corner) with the other unit. This concept draws parallels to the movement of a queen in chess, which can traverse any number of squares in vertical, horizontal, or diagonal directions. For a dataset with n observations, this leads to an n x n matrix, commonly referred to as the weight or link matrix, which captures the pairwise spatial associations across the data. In this matrix, a '1' denotes neighboring spatial units, while a '0' signifies non-neighboring units. For this report, we will consistently use this weight matrix. However, it's generally advisable to test multiple weight matrices to ensure that our findings aren't solely influenced by the specific matrix chosen.

To determine if the spatial autocorrelation, as measured by Moran's I, is significant, we perform random permutations, which tests the null hypothesis that there is no spatial autocorrelation against the alternative hypothesis that there is significant spatial autocorrelation. In this process, the observed value of Moran's I is compared to a distribution of Moran's I values generated from many random permutations of the spatial data. To elaborate, the observed house price values undergo 999 random shuffles, producing a corresponding 999 Moran’s I values from these permutations. Next, we arrange the 1000 Moran’s I values in decreasing order to determine the position of the Moran’s I value for the observed house price variable in relation to the values from the random permutations. If our observed value is in the extreme ends of this distribution (either very high or very low), we reject the null hypothesis, indicating that the observed spatial autocorrelation is significant.

While Moran's I provides a global measure of spatial autocorrelation, it doesn't tell us where the local clusters and local spatial outliers are. Local spatial autocorrelation, like Local Indicators of Spatial Association (LISA), allow us to identify specific areas of significant clustering or dispersion. For each block, by looking at the deviations of its housing value from the mean housing values(zi) and that of its neighbors(zj), spatial weights between i and j, and the total number of observations(n), we can then determine if it's part of a significant cluster of similar values (high-high or low-low) or if it's an outlier in its neighborhood (high-low or low-high). To be more specific, a positive value indicates that housing value of block i is surrounded by blocks with similar housing values, either all high or all low. A negative value indicates that housing value of block i a positive value indicates that housing value of block I is surrounded by blocks with similar housing values, either all high or all low. Also, a value near zero indicates no significant local spatial autocorrelation.

Significance tests for local spatial autocorrelation are based on Monte Carlo permutation approach, which tests the null hypothesis that there is no local spatial autocorrelation at location I against the alternative hypothesis that the local spatial autocorrelation is significant. During the permutation, the housing values of each block will be randomly shuffled for 999 times, based on which we calculate the new Moran’s I value for every location for each permutation. The value of Moran’s I at location i for the original dataset is ranked relative to the list of the values produced by the reshufflings. When values of the Moran’s I at location i for the original dataset are very low or very high relative to the list of results produced by the shuffling procedure, they are significant. A pseudo significance level can be ascertained by observing the rank of the observed value in comparison to the permuted outcomes. For instance, if the value of Moran’s I at location i from the original configuration ranks as the 88th highest out of 999 permutations, it's viewed as a 88 in 1000 event with a pseudosignificance of p ~ 0.088. 

## b) A Review of OLS Regression and Assumptions
In our OLS regression model, we found that the predictors—PCTVACANT, PCTSINGLES, PCTBACHMOR, and LNNBELPOV100, are significantly correlated with the dependent variable LNMEDHVAL, and we rejected the null hypothesis that all beta coefficients are zero. We thoroughly assessed the regression assumptions for OLS regression, including the normality of residuals, homoscedasticity, absence of multicollinearity, linearity between the dependent variable y and each predictor x, and the independence of observations. While some assumptions are met in our model, there are some that must be challenged. We examined the spatial autocorrelation simply by plotting choropleth graphs, revealing noticeable spatial autocorrelation between the dependent variable and one of the predictors. Furthermore, the standardized regression residuals map exhibits some degree of spatial autocorrelation, challenging the assumption that residuals are random, and the observations are independent. 

Therefore, we are using statistical method to test the spatial autocorrelation, which is called Moran’s I value, indicating whether spatial autocorrelation exists or not. Moran’s I value is between -1 to +1, and the more positive (approaching to +1) the number is, the stronger positive spatial autocorrelations there would be, and more negative (approaching to -1) the more negative spatial autocorrelations. 

To further assess spatial autocorrelation within OLS residuals, we employed an additional method that involves regressing these residuals against those of nearby locations. For this analysis, two distinct approaches were used to define neighbors, the Rook Neighbor and the Queen Neighbor Matrix. While rook neighbor method only considers the immediate neighbors in the four cardinal directions with directly shared boundaries, queen neighbor accounts for neighbors with shared corners or intersections, thereby considering a broader range of spatial relationships. Generally, the Queen Neighbor Matrix is preferred due to its broader scope. 

In this process, the resulting residuals are calculated as the average of the residuals from these neighboring locations. We then conducted a linear regression using OLS residuals against these averaged neighbor residuals. The focus of this analysis was on the significance of the relationship and the magnitude of the slope coefficient. A p-value less than 0.05 would lead us to reject the null hypothesis, thereby confirming the presence of spatial autocorrelation. 

Heteroscedasticity is defined as the dispersion of residuals varies by level of predicted variable. To test the assumption of homoscedasticity, we have three tests that can be used in R: the Breusch-Pagan Test, Koenker-Bassett Test, and the White test. The null hypothesis here is that of homoscedasticity. If the p-value is less than 0.05, then we can reject the null hypothesis for the alternate hypothesis of heteroscedasticity. 

Another assumption, normality of errors, meaning that the errors should be random noise, and they also should be normally distributed. The Jarque-Bera test in R examines the null hypothesis that the residuals are from a normal distribution, whereas the null hypothesis is that the errors are normal while the alternative hypothesis of non-normality. 

# **Results**
##a)Spatial autocorrelation
## Global Moran's I
```{r}
## Defining Neighbors
queen <- poly2nb(data_geom , queen=TRUE)
## Global Moran's I
queenlist<-nb2listw(queen, style = 'W')
moran(data_geom$LNMEDHVAL, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I` 

```

```{r warning=FALSE, message=FALSE, cache=FALSE}
moranMC<-moran.mc(data_geom$LNMEDHVAL, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations
moranMC
```
```{r warning=FALSE, message=FALSE, cache=FALSE}
moranMCres<-moranMC$res
hist(moranMCres, freq=10000000, nclass=100)   #Draws distribution of Moran's I's calculated from randomly permuted values
# Here, we draw a red vertical line at the observed value of our Moran's I
abline(v=moran(data_geom$LNMEDHVAL, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I`, col='red')  
```

```{r warning=FALSE, message=FALSE, cache=FALSE}
#Create Moran plot (lagged value against observed value)
moran.plot(data_geom$LNMEDHVAL, queenlist) 
```
## Local Moran's I
```{r warning=FALSE, message=FALSE, cache=FALSE}
#Run local moran's I (LISA) 
LISA<-localmoran(data_geom$LNMEDHVAL, queenlist)
head(LISA)
df.LISA <-cbind(data_geom, as.data.frame(LISA))
```
```{r warning=FALSE, message=FALSE, cache=FALSE}
moranSig.plot<-function(df,listw, title){
  local<-localmoran(x=df$LNMEDHVAL, listw=listw, zero.policy = FALSE)
  moran.map<-cbind(df, local)
  #Here, col='Pr.z....E.Ii..' is the name of the column in the dataframe df.LISA that we're trying to plot. This variable name might change based on the version of the package.
  tm<-tm_shape(moran.map)+
    tm_borders(col='white')+
    tm_fill(style='fixed', col='Pr.z....E.Ii..', breaks=c(0,0.001, 0.01, 0.05, 1), title= 'p-value', palette = '-BuPu')+
    tm_layout(frame = FALSE, title = title)
  print(tm)
}
moranSig.plot(df.LISA, queenlist, 'p-value')
```
## Regression Analysis: OLS Regression



```{r warning=FALSE, message=FALSE, cache=FALSE}
reg<-lm(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, data=data_geom)
summary(reg)
#Prints the log likelihood
logLik(reg)                  
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest(reg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest(reg)       
#Prints the results of the White Test to assess whether heteroscedasticity is present (package: whitestrap)
white_test(reg)   
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(reg$residuals)
```



```{r warning=FALSE, message=FALSE, cache=FALSE}
standardised<-rstandard(reg)
resnb<-sapply(queen, function(x) mean(standardised[x]))

data_geom$standardised <- standardised    #creating a new variable in the shapefile shp.
OLS.Residuals.Map<-tm_shape(data_geom)+
  tm_fill(col='standardised', style='quantile', title='Standardized OLS Residuals', 
          palette ='Blues')+
  tm_layout(frame=FALSE, title = 'Standardised OLS Residuals')
OLS.Residuals.Map

```



```{r warning=FALSE, message=FALSE, cache=FALSE}
#Regressing residuals on their nearest neighbors.
res.lm <- lm(formula=standardised ~ resnb)
summary(res.lm)
```


```{r warning=FALSE, message=FALSE, cache=FALSE}
moran.mc(standardised, queenlist, 999, alternative="two.sided")
moran.plot(standardised, queenlist)
```
## Regression Analysis: Spatial Lag Regression
```{r warning=FALSE, message=FALSE, cache=FALSE}
lagreg<-lagsarlm(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, data=data_geom, queenlist)
summary(lagreg)
LR.Sarlm(lagreg, reg) #Here lagreg is the SL output; reg is the OLS output
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lagreg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lagreg)       
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(lagreg$residuals)
```
```{r warning=FALSE, message=FALSE, cache=FALSE}
#Spatial lag map
model_residuals <- residuals(lagreg)
#standardized2 <- rstandard(lagreg)
resnb<-sapply(queen, function(x) mean(model_residuals[x]))

data_geom$residuals <- model_residuals    #creating a new variable in the shapefile shp.
SpatiallagResiduals_Map<-tm_shape(data_geom)+
  tm_fill(col='residuals', style='quantile', title='Standardized Spatial Lag Residuals', palette ='Blues')+
  tm_layout(frame=FALSE, title = 'Standardised Spatial Lag Residuals')
SpatiallagResiduals_Map
```



```{r warning=FALSE, message=FALSE, cache=FALSE}
reslag<-lagreg$residuals
lagMoranMc<-moran.mc(reslag, queenlist,999, alternative="two.sided")
lagMoranMc
moran.plot(reslag, queenlist)
```

## Regression Analysis: Spatial Error Regression


```{r warning=FALSE, message=FALSE, cache=FALSE}
errreg<-errorsarlm(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, data=data_geom, queenlist)
reserr<-residuals(errreg)
errresnb<-sapply(queen, function(x) mean(reserr[x]))
summary(errreg)
LR.Sarlm(errreg, reg)
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(errreg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(errreg)       
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(errreg$residuals)
```

```{r warning=FALSE, message=FALSE, cache=FALSE}
#Spatial error map
model_residuals2 <- residuals(errreg)
#standardized2 <- rstandard(lagreg)
resnb<-sapply(queen, function(x) mean(model_residuals[x]))

data_geom$residuals <- model_residuals2    #creating a new variable in the shapefile shp.
SpatialerrorResiduals_Map<-tm_shape(data_geom)+
  tm_fill(col='residuals', style='quantile', title='Standardized Spatial Error Residuals', palette ='Blues')+
  tm_layout(frame=FALSE, title = 'Standardised Spatial Error Residuals')
SpatialerrorResiduals_Map
```



```{r warning=FALSE, message=FALSE, cache=FALSE}
errMoranMc<-moran.mc(reserr, queenlist, 999, alternative="two.sided")
errMoranMc
moran.plot(reserr, queenlist)
```

## c)	Spatial Lag and Spatial Error Regression Results
The spatial lag model yields several key insights. Firstly, the W_LNMEDHAL is 0.6511, signifying a substantial and significant spatial relationship: median house values in one area are moderately strongly linked with those in adjacent areas. Moreover, the predictors, including LNNBELPOV, PCTBACHMOR, PCTSINGLES, and PCTVACNT, have p-values far below the 0.05 threshold, confirming their statistical significance. When comparing with the Ordinary Least Squares (OLS) model, there's a slight uptick in the p-values for these predictors, hinting at a greater likelihood that the associations between median housing values and these predictors could be attributed to chance.

At the same time, as suggested by the result of the Breusch-Pagan test, the p value is way much smaller than 0.05, which suggests that we reject the null hypothesis and acknowledge the presence of heteroscedasticity.

In comparing the OLS and Spatial Lag regressions, we turn to the Akaike Information Criterion (AIC), Log Likelihood, and the Likelihood Ratio Test. The Spatial Lag regression yields an AIC of 525.48—markedly lower than the OLS regression's AIC of 1435—and a Log Likelihood of -255.74, which surpasses the OLS regression's -711.49. The Likelihood Ratio Test's p-value is well below 0.05, leading us to discard the null hypothesis that the spatial lag model does not offer a better fit than the OLS model. Additionally, the Moran’s I value for the spatial lag model is a minimal -0.082412, indicating significantly reduced spatial autocorrelation in the residuals compared to the OLS regression. This evidence suggests that the Spatial Lag model provides a better fit.

Now we will also look at the results of Spatial Error regression. Here, we see that lambda has the value of 0.81492 and is significant, indicating that the median house value in an area has a strong relationship with median house value in surrounding areas. Also, the corresponding p-value of all the predictors are smaller than 0.05, indicating that all of them are significant. All predictors maintain p-values under 0.05, underscoring their significance. Yet, relative to the OLS regression, the predictors' p-values have risen, signaling a diminution in the strength of their statistical significance.

At the same time, as suggested by the result of the Breusch-Pagan test, the p value is way much smaller than 0.05, pointing to heteroscedasticity within the Spatial Lag regression residuals.

When we consider the AIC and Log Likelihood for the Spatial Error regression, we find an AIC of 754.985 and a Log Likelihood of -372.6904—both figures are more favorable than those from the OLS model. The Moran’s I value for the Spatial Error regression is just -0.094532, indicating even weaker spatial autocorrelation compared with the OLS model. Given that the Spatial Lag model has the lowest AIC, it stands as the best-performing model.

Lastly, in comparing the Spatial Lag and Spatial Error models, we note that the AIC for the Spatial Lag is lower than that of the Spatial Error Model, suggesting the former as the more predictive model. Since the Spatial Lag and Spatial Error models are not nested, direct comparison using the log-likelihood ratio is not applicable, thus we rely on the AIC for this assessment.
